{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# DATA GENERATION CLASS (for transparency)\n",
    "# ===============================================\n",
    "# This class shows how our training data was generated.\n",
    "# The actual data used in this experiment is loaded from a pickle file below.\n",
    "\n",
    "class AdjacencyMatrixGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate adjacency matrices for directed and undirected graphs.\n",
    "    This code is included for transparency - actual data is loaded from pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.matrices = []\n",
    "        self.labels = []\n",
    "\n",
    "    def generate_symmetric_matrix(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate a symmetric adjacency matrix (undirected graph).\"\"\"\n",
    "        matrix = np.random.randint(0, 2, size=(n, n))\n",
    "        matrix = np.triu(matrix) + np.triu(matrix, 1).T\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        return matrix\n",
    "\n",
    "    def generate_non_symmetric_matrix(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate a non-symmetric adjacency matrix (directed graph).\"\"\"\n",
    "        matrix = np.random.randint(0, 2, size=(n, n))\n",
    "        np.fill_diagonal(matrix, 0)\n",
    "        if np.array_equal(matrix, matrix.T):\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    if matrix[i, j] == matrix[j, i]:\n",
    "                        matrix[i, j] = 1 - matrix[i, j]\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "        return matrix\n",
    "\n",
    "    def generate_dataset(self, n: int, num_symmetric: int, num_non_symmetric: int):\n",
    "        \"\"\"Generate a dataset of adjacency matrices.\"\"\"\n",
    "        self.matrices = []\n",
    "        self.labels = []\n",
    "\n",
    "        for _ in range(num_symmetric):\n",
    "            matrix = self.generate_symmetric_matrix(n)\n",
    "            self.matrices.append(matrix)\n",
    "            self.labels.append(0)\n",
    "\n",
    "        for _ in range(num_non_symmetric):\n",
    "            matrix = self.generate_non_symmetric_matrix(n)\n",
    "            self.matrices.append(matrix)\n",
    "            self.labels.append(1)\n",
    "\n",
    "    def save_to_pickle(self, filename: str):\n",
    "        \"\"\"Save the generated matrices and labels to a pickle file.\"\"\"\n",
    "        data = {\n",
    "            'matrices': self.matrices,\n",
    "            'labels': self.labels,\n",
    "            'matrix_size': len(self.matrices[0]) if self.matrices else 0\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        print(f\"Saved {len(self.matrices)} matrices to {filename}\")\n",
    "\n",
    "    def load_from_pickle(self, filename: str):\n",
    "        \"\"\"Load matrices and labels from a pickle file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        self.matrices = data['matrices']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "        print(f\"Loaded {len(self.matrices)} matrices from {filename}\")\n",
    "\n",
    "    def get_flattened_data(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get matrices flattened into vectors for neural network input.\"\"\"\n",
    "        if not self.matrices:\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        flattened_matrices = np.array([matrix.flatten() for matrix in self.matrices])\n",
    "        labels_array = np.array(self.labels)\n",
    "\n",
    "        return flattened_matrices, labels_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luis Hernandez, Roberto Palacios\n",
    "# Note: this code was partially generated by AI (Grok 3)\n",
    "\n",
    "# Sigmoid activation function to map values to (0, 1)\n",
    "def sigmoid(x):\n",
    "    # Compute 1 / (1 + e^-x) element-wise\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid for backpropagation, note that this takes in activated output and not raw input\n",
    "def sigmoid_derivative_from_output(x):\n",
    "    # Compute sigmoid(x) * (1 - sigmoid(x)) for the gradient\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Neural Network class to classify graphs\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initializing weights and biases with small random values\n",
    "        # input_size = 16 (4x4 flattened), hidden_size = number of hidden neurons, output_size = 1\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden weights\n",
    "        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output weights\n",
    "        self.b2 = np.zeros((1, output_size))  # Output biases\n",
    "    # Perform forward propagation through the network\n",
    "    def forward(self, X):\n",
    "        # Compute hidden layer input: X * W1 + b1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        # Apply sigmoid activation to hidden layer\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        # Compute output layer input: a1 * W2 + b2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        # Apply sigmoid activation to output layer\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        # Return final output\n",
    "        return self.a2\n",
    "\n",
    "    # Backward propagation to update weights & biases\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Compute error at output layer, true means predicted\n",
    "        self.error = y - output\n",
    "        # Compute output layer gradient, error * sigmoid derivative\n",
    "        self.delta2 = self.error * sigmoid_derivative_from_output(output)\n",
    "\n",
    "        # Propagate error to hidden layer: delta2 * W2^T\n",
    "        self.error_hidden = np.dot(self.delta2, self.W2.T)\n",
    "        # Compute hidden layer gradient: error_hidden * sigmoid derivative\n",
    "        self.delta1 = self.error_hidden * sigmoid_derivative_from_output(self.a1)\n",
    "\n",
    "        # Update weights & biases using gradient descent\n",
    "        self.W2 += learning_rate * np.dot(self.a1.T, self.delta2)  # Update hidden to output weights\n",
    "        self.b2 += learning_rate * np.sum(self.delta2, axis=0, keepdims=True)  # Update output biases\n",
    "        self.W1 += learning_rate * np.dot(X.T, self.delta1)  # Update input to hidden weights\n",
    "        self.b1 += learning_rate * np.sum(self.delta1, axis=0, keepdims=True)  # Update hidden biases\n",
    "\n",
    "    # Train the network for the specified number of epochs\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        errors = []  # List to store mean squared error per epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass to get predictions\n",
    "            output = self.forward(X)\n",
    "            # Backward pass to update parameters\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            # Compute mean squared error for this epoch\n",
    "            mse = np.mean(np.square(y - output))\n",
    "            errors.append(mse)\n",
    "            # Print progress every 1000 epochs\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, MSE: {mse:.6f}\")\n",
    "        # Return list of errors for plotting\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for neural network\n",
    "input_size = 16  # 4x4 matrix flattened into a 16-element vector\n",
    "hidden_size = 32  # Number of neurons in the hidden layer, adjustable\n",
    "output_size = 1   # Single output neuron: 0 for undirected, 1 for directed\n",
    "learning_rate = 0.01  # Step size for gradient descent\n",
    "epochs = 10000  # Number of training iterations\n",
    "num_samples = 10  # Total number of graph samples (5 directed, 5 undirected)\n",
    "\n",
    "# ===============================================\n",
    "# LOAD TRAINING DATA\n",
    "# ===============================================\n",
    "# Load data using our class methods\n",
    "generator = AdjacencyMatrixGenerator()\n",
    "\n",
    "# Option 1: Load from pickle file (recommended for reproducibility)\n",
    "try:\n",
    "    generator.load_from_pickle(\"neural_network_training_data.pkl\")\n",
    "    print(\"Using pre-generated dataset for reproducible results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Pickle file not found. Generating new dataset...\")\n",
    "    generator.generate_dataset(n=4, num_symmetric=num_samples//2, num_non_symmetric=num_samples//2)\n",
    "    generator.save_to_pickle(\"neural_network_training_data.pkl\")\n",
    "\n",
    "# Get data ready for neural network\n",
    "X, y = generator.get_flattened_data()\n",
    "y = y.reshape(-1, 1)  # Reshape for neural network compatibility\n",
    "\n",
    "print(f\"Dataset ready: {len(X)} samples, {X.shape[1]} features each\")\n",
    "\n",
    "# Initialize the neural network with specified architecture\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "# Train the network and collect error history\n",
    "errors = nn.train(X, y, epochs, learning_rate)\n",
    "\n",
    "# Plot the mean squared error over training epochs\n",
    "plt.plot(errors)  # Plot error values\n",
    "plt.xlabel('Epoch')  # Label x-axis\n",
    "plt.ylabel('Mean Squared Error')  # Label y-axis\n",
    "plt.title('Training Error vs Epoch')  # Set plot title\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Test the trained network on the dataset\n",
    "print(\"\\nTesting network:\")\n",
    "for i in range(num_samples):\n",
    "    # Get prediction for each sample\n",
    "    prediction = nn.forward(X[i:i+1])[0][0]\n",
    "    # Get true label\n",
    "    actual = y[i][0]\n",
    "    # Print predicted and actual values\n",
    "    print(f\"Sample {i+1}: Predicted: {prediction:.3f}, Actual: {actual}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
